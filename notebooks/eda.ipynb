{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data access and visualization\n",
    "\n",
    "The first thing one must do is to load the data and being able to visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import core.dataloader as crloader\n",
    "import core.data_plots as crplt\n",
    "import core.preprocesses as crpre\n",
    "# Load the data\n",
    "data = crloader.load_data(data_path='../data/physionet.org/files/ptb-xl/1.0.2',\n",
    "                       sampling_rate=100)\n",
    "print(\"\\nData loaded.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access a specific patient's data\n",
    "patient_id = 5678\n",
    "ecg_ids = crloader.get_patient_id_ecg_ids(patient_id=patient_id,\n",
    "                                          annotations=data['train']['annotations'])\n",
    "\n",
    "print(f\"Patient {patient_id} has {len(ecg_ids)} ECGs.\")\n",
    "ecg_id, ecg_date = ecg_ids[-1]  # most recent\n",
    "\n",
    "signals = crloader.get_signal_from_ecg_id(ecg_id=ecg_id,\n",
    "                                          raw_data=data['train']['data'],\n",
    "                                          channel=-1)\n",
    "\n",
    "annots = crloader.get_annotations_from_ecg_id(ecg_id=ecg_id,\n",
    "                                              annotations=data['train']['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the ECG signal for all channels and annotations\n",
    "data_display = crplt.plot_ecg_channels(raw_data=data['train']['data'][ecg_id],\n",
    "                                       title=f\"ECG ID {ecg_id} from {ecg_date}\")\n",
    "\n",
    "print(annots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocess\n",
    "\n",
    "By preprocessing the signals, one can make them smoother, remove outliers, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "\n",
    "A common pre-process when working with signals is smoothing/filtering. That allows to remove some outliers and noise from the signal for a better analysis.\n",
    "\n",
    "Some of the most used signal filtering techniques are:\n",
    "- Savitzky-Golay filter\n",
    "- Gaussian filter\n",
    "- Median filter\n",
    "- Low-pass filter\n",
    "- High-pass filter\n",
    "- Butterworth filter (band-pass filter)\n",
    "- Convolution filter\n",
    "\n",
    "The biggest challenge of filtering is the manual tunning. Finding the right parameters is a empirical work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = 0\n",
    "original_signal = signals[:, channel]\n",
    "savgol_ecg = crpre.smooth_signal_savgol(ecg_signal=original_signal,\n",
    "                                        window_length=5,\n",
    "                                        polyorder = 2)\n",
    "crplt.plot_filtered_signal(ecg_signal=original_signal,\n",
    "                           smoothed_ecg=savgol_ecg,\n",
    "                           title=\"Savitzky-Golay filter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_ecg = crpre.smooth_signal_gaussian(ecg_signal=original_signal, sigma=3)\n",
    "crplt.plot_filtered_signal(ecg_signal=original_signal,\n",
    "                           smoothed_ecg=gaussian_ecg,\n",
    "                           title=\"Gaussian filter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_ecg = crpre.smooth_signal_median(ecg_signal=original_signal, kernel_size=3)\n",
    "crplt.plot_filtered_signal(ecg_signal=original_signal,\n",
    "                           smoothed_ecg=median_ecg,\n",
    "                           title=\"Median filter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowcut = 45\n",
    "lowpass_ecg = crpre.smooth_signal_lowpass(ecg_signal=original_signal,\n",
    "                                          sample_rate=100,\n",
    "                                          order_filter=5,\n",
    "                                          cut=lowcut)\n",
    "crplt.plot_filtered_signal(ecg_signal=original_signal,\n",
    "                           smoothed_ecg=lowpass_ecg,\n",
    "                           title=f\"Low-pass filter at {lowcut} Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highcut = 0.5\n",
    "highpass_ecg = crpre.smooth_signal_highpass(ecg_signal=original_signal,\n",
    "                                          sample_rate=100,\n",
    "                                          order_filter=5,\n",
    "                                          cut=highcut)\n",
    "crplt.plot_filtered_signal(ecg_signal=original_signal,\n",
    "                           smoothed_ecg=lowpass_ecg,\n",
    "                           title=f\"High-pass filter at {highcut} Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowcut = 0.5  # avoid the breathing noise\n",
    "highcut = 45  # avoid power-line noise\n",
    "band_ecg = crpre.smooth_signal_butterworth(ecg_signal=original_signal,\n",
    "                                           sample_rate=100,\n",
    "                                           order_filter=5,\n",
    "                                           lowcut=lowcut,\n",
    "                                           highcut=highcut)\n",
    "crplt.plot_filtered_signal(ecg_signal=original_signal,\n",
    "                           smoothed_ecg=band_ecg,\n",
    "                           title=f\"Butterworth filter ({lowcut}Hz - {highcut}Hz)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 7\n",
    "conv_ecg = crpre.smooth_signal_convolution(ecg_signal=original_signal,\n",
    "                                           kernel=kernel)\n",
    "crplt.plot_filtered_signal(ecg_signal=original_signal,\n",
    "                           smoothed_ecg=conv_ecg,\n",
    "                           title=f\"Convolution filter (kernel wide {kernel})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, tunning a filter is hard work. As an example, I show the influence of difference frequency cuts on a low-filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowpass_ecgs = []\n",
    "cutoffs = []\n",
    "for lowcut in range(40, 50, 2):\n",
    "    lowpass_ecg = crpre.smooth_signal_lowpass(ecg_signal=original_signal,\n",
    "                                            sample_rate=100,\n",
    "                                            order_filter=5,\n",
    "                                            cut=lowcut)\n",
    "    lowpass_ecgs.append(lowpass_ecg)\n",
    "    cutoffs.append(f\"{lowcut}Hz\")\n",
    "\n",
    "crplt.plot_filtered_signals(ecg_signal=original_signal,\n",
    "                           smoothed_ecgs=lowpass_ecgs,\n",
    "                           labels=cutoffs,\n",
    "                           title=\"Low-pass filter search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An application of the filtering is to remove the baseline wander.\n",
    "\n",
    "Baseline wander is a typical artifact that corrupts the ECG. It can be caused by a variety of noise sources including respiration, body movements, and poor electrode contact. Its spectral content is usually confined to frequencies below 0.5 Hz.\n",
    "\n",
    "The majority of baseline wander removal techniques can change the ECG and compromise its clinical relevance. For that reason, it is not a easy process.\n",
    "\n",
    "A very basic baseline wander estimator was implemented using a sequence of median filter with different kernel sizes. The kernel size is estimated based on the sampling rate and the window duration in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wander = crpre.estimate_baseline_wander(ecg_signal=original_signal,durations=[0.5, 2], sample_rate=100)\n",
    "rem_wander_ecg = crpre.remove_baseline_wander(ecg_signal=original_signal,durations=[0.5, 2], sample_rate=100)\n",
    "\n",
    "crplt.plot_filtered_signals(ecg_signal=original_signal,\n",
    "                           smoothed_ecgs=[wander, rem_wander_ecg],\n",
    "                           labels=['estimated wander', 'filtered'],\n",
    "                           title=\"Remove baseline wander\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the ECG signal, there is some basic analysis that one can do.\n",
    "One of the most relevant information from an ECG is to look at the [QRS complex](https://en.wikipedia.org/wiki/QRS_complex).\n",
    "In layman terms:\n",
    "- R peak are the highest peaks\n",
    "- Q peaks are the minimum peak before the R peak\n",
    "- S peaks are the minimum peak after the R peak\n",
    "\n",
    "From the R peaks one can estimate the heart rate.\n",
    "\n",
    "Some of the most used detectors are:\n",
    "- Pan and Tompkins\n",
    "- Hamilton\n",
    "- Christov\n",
    "- Stationary Wavelet Transform\n",
    "- Two Moving Average\n",
    "\n",
    "And you can find an implementation [here](https://github.com/berndporr/py-ecg-detectors).\n",
    "After trying it, I was not satisfied with the results. In most of the cases the R peaks were completly off.\n",
    "\n",
    "I implemented my own [Pan and Tompkins QRS complex detector](https://en.wikipedia.org/wiki/Pan%E2%80%93Tompkins_algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.pan_tompkins import PanTompkinsQRS\n",
    "peak_detector = PanTompkinsQRS(signal=original_signal, sample_rate=100, window_size=0.15)\n",
    "\n",
    "crplt.plot_signal(signal=peak_detector.band_pass_sgn,\n",
    "                  xlabel=\"Samples\",\n",
    "                  ylabel=\"Amplitude\",\n",
    "                  title=\"Bandpassed signal\")\n",
    "\n",
    "crplt.plot_signal(signal=peak_detector.mov_win_sgn,\n",
    "                  xlabel=\"Samples\",\n",
    "                  ylabel=\"Amplitude\",\n",
    "                  title=\"Moving window integrated signal\")\n",
    "\n",
    "peak_detector.find_r_peaks()\n",
    "\n",
    "crplt.plot_signal_and_rpeaks(signal=original_signal,\n",
    "                             rpeaks_loc=peak_detector.tuned_peaks,\n",
    "                             xlabel=\"Samples\",\n",
    "                             ylabel=\"Amplitude\",\n",
    "                             title=\"R peaks\")\n",
    "\n",
    "heart_bpm, heart_var = peak_detector.estimate_heartrate()\n",
    "print(f\"Heart rate: {heart_bpm:.2f} +- {heart_var:.2f} bpm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data split check up\n",
    "\n",
    "I want to see how the classes are distributed in the 3 different splits. I'm using the splits suggested by `Physionet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.utils import calculate_distribution\n",
    "\n",
    "datasets = [\"train\", \"val\", \"test\"]\n",
    "class_counts = {}\n",
    "class_percentages = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    class_counts[dataset], class_percentages[dataset] = calculate_distribution(data[dataset][\"labels\"],\n",
    "                                                                                       use_combo=False)\n",
    "\n",
    "num_classes = len(class_counts[\"train\"])\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "crplt.plot_data_distribution(class_counts, class_percentages)\n",
    "\n",
    "datasets = [\"train\", \"val\", \"test\"]\n",
    "class_counts = {}\n",
    "class_percentages = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    class_counts[dataset], class_percentages[dataset] = calculate_distribution(data[dataset][\"labels\"],\n",
    "                                                                                       use_combo=True)\n",
    "\n",
    "num_classes = len(class_counts[\"train\"])\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "crplt.plot_data_distribution(class_counts, class_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds_type in datasets:\n",
    "    single_label = 0\n",
    "    norm_label = 0\n",
    "    multi_label = 0\n",
    "    for key, perc in class_percentages[ds_type].items():\n",
    "        if \" \" in key:\n",
    "            multi_label += perc\n",
    "        else:\n",
    "            single_label += perc\n",
    "            if \"norm\" == key.lower():\n",
    "                norm_label += perc\n",
    "    print(f\"Dataset type: {ds_type}\")\n",
    "    print(f\"\\tPercentage of single labels: {single_label:.2f}%\")\n",
    "    print(f\"\\t\\tNORM contribution: {norm_label:.2f}%\")\n",
    "    print(f\"\\t\\tNon NORM contribution: {single_label - norm_label:.2f}%\")\n",
    "    print(f\"\\tPercentage of multi labels: {multi_label:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that we have 5 unique labels and 22 out of 31 possible combinations of labels.\n",
    "\n",
    "There are few other things to notice:\n",
    "- there are data points without a class\n",
    "- the data is balanced between splits\n",
    "- the contribution of single labels is dominant\n",
    "- the single label NORM represents +- 40% of the data in every split\n",
    "- the data doesn't have a long tail distribution (not assuming the combos)\n",
    "\n",
    "Since this is a health use case, `sex` and `age` are often important factors. Let's check if that data is also properly distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "for ds_type in datasets:\n",
    "    sex_count = Counter(data[ds_type][\"annotations\"].sex)\n",
    "    total = sum(sex_count.values())\n",
    "    print(f\"Dataset type: {ds_type}\")\n",
    "    print(f\"\\tMale percentage: {sex_count['male']/total:.2%} ({sex_count['male']})\")\n",
    "    print(f\"\\tFemale percentage: {sex_count['female']/total:.2%} ({sex_count['female']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there is a slightly prevalence of `male`, the `sex` attribute is properly distributed between datasets.\n",
    "\n",
    "Let's analyze the `age` distribution per `sex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds_type in datasets:\n",
    "    print(f\"Dataset type: {ds_type}\")\n",
    "    crplt.plot_distribution_age_sex(data[ds_type][\"annotations\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `age` distribution per `sex` in the different datasets is very similar.\n",
    "\n",
    "There 2 points to note:\n",
    "- there are clearly data points with unrealistic age (+- 300)\n",
    "- we have more `female` data in the extremes and more `male` data in the centre\n",
    "\n",
    "This last points alert us to check for bias in the results later on.\n",
    "\n",
    "\n",
    "Finally, lets check the `diagnostic` distribution per `sex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds_type in datasets:\n",
    "    print(f\"Dataset type: {ds_type}\")\n",
    "    crplt.plot_distribution_diagnostic_sex(data[ds_type][\"annotations\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `diagnostic` distribution per `sex` in the different datasets is very similar.\n",
    "We can also observe that `female` has more `NORM` diagnostics. That means that the we have less females in the data and they are mostly healthy. One can expect to be harder to detect problems with a model on females. Something to confirm later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "\n",
    "In this multi-label problem, we could see that the data is well distributed between the splits considering `diagnostic`, `sex` and `age`.\n",
    "\n",
    "But we also noticed that the label `NORM` is dominant and represents +- 35% of the data in a split and the next most representative label is +- 20%.\n",
    "\n",
    "The challenge now is to check if we can augment the data such that the the models would perform better.\n",
    "In theory, the best results are achieved with a labels distribution more balance inside the train split. In practiced, they may not be true and only by training and testing the models one can know.\n",
    "\n",
    "I will focus the augmentation on the cases with one label only.\n",
    "Once the data is resampled, I will add to the data all the multi labeled cases.\n",
    "\n",
    "I will have at my disposal 6 data distributions to evaluate:\n",
    "- use the data as it is\n",
    "- use only the over/under sample the data with one label\n",
    "- over/under sample the data with one label and add the multi labeled cases\n",
    "- use only the multi labeled cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recap of the original distribution\n",
    "datasets = [\"train\", \"val\", \"test\"]\n",
    "class_counts = {}\n",
    "class_percentages = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    class_counts[dataset], class_percentages[dataset] = calculate_distribution(data[dataset][\"labels\"],\n",
    "                                                                                       use_combo=False)\n",
    "crplt.plot_data_distribution(class_counts, class_percentages)\n",
    "\n",
    "num_classes = len(class_counts[\"train\"])\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Classes: {class_counts['train']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.data_augmentation import DataAugmentor\n",
    "aug = DataAugmentor()\n",
    "\n",
    "# Single label only\n",
    "new_data, new_labels = aug.get_single_label_only(data[\"train\"][\"data\"], data[\"train\"][\"labels\"])\n",
    "class_counts[\"train\"], class_percentages[\"train\"] = calculate_distribution(new_labels)\n",
    "crplt.plot_data_distribution(class_counts, class_percentages)\n",
    "\n",
    "# Multi labels only\n",
    "new_data, new_labels = aug.get_multi_label_only(data[\"train\"][\"data\"], data[\"train\"][\"labels\"])\n",
    "class_counts[\"train\"], class_percentages[\"train\"] = calculate_distribution(new_labels)\n",
    "crplt.plot_data_distribution(class_counts, class_percentages)\n",
    "\n",
    "# Single label under sampled\n",
    "new_data, new_labels = aug.get_undersampled(data[\"train\"][\"data\"], data[\"train\"][\"labels\"], add_multi=False)\n",
    "class_counts[\"train\"], class_percentages[\"train\"] = calculate_distribution(new_labels)\n",
    "crplt.plot_data_distribution(class_counts, class_percentages)\n",
    "\n",
    "# Single label under sampled and multi label\n",
    "new_data, new_labels = aug.get_undersampled(data[\"train\"][\"data\"], data[\"train\"][\"labels\"], add_multi=True)\n",
    "class_counts[\"train\"], class_percentages[\"train\"] = calculate_distribution(new_labels)\n",
    "crplt.plot_data_distribution(class_counts, class_percentages)\n",
    "\n",
    "# Single label over sampled\n",
    "new_data, new_labels = aug.get_oversampled(data[\"train\"][\"data\"], data[\"train\"][\"labels\"], add_multi=False)\n",
    "class_counts[\"train\"], class_percentages[\"train\"] = calculate_distribution(new_labels)\n",
    "crplt.plot_data_distribution(class_counts, class_percentages)\n",
    "\n",
    "# Single label over sampled and multi label\n",
    "new_data, new_labels = aug.get_oversampled(data[\"train\"][\"data\"], data[\"train\"][\"labels\"], add_multi=True)\n",
    "class_counts[\"train\"], class_percentages[\"train\"] = calculate_distribution(new_labels)\n",
    "crplt.plot_data_distribution(class_counts, class_percentages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
